{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99n367jq27",
   "metadata": {},
   "source": "# Notebook 3: Select Normalization Method\n\n## Overview\n\nThis notebook helps you **evaluate and select the best normalization method** for your FTIR spectral data. Normalization is the final preprocessing step that makes spectra directly comparable by scaling intensities appropriately.\n\n### What You'll Learn\n\n1. How to apply denoising and baseline correction together\n2. How to handle atmospheric interference (CO₂, H₂O)\n3. How to evaluate 7+ normalization methods\n4. How to select the best method using classification performance\n\n### Why Normalize?\n\nNormalization addresses:\n- **Path length variations**: Different sample thicknesses\n- **Concentration differences**: Varying amounts of material\n- **Instrument variations**: Detector sensitivity changes\n- **Scatter effects**: Light scattering from particles\n\n### Available Normalization Methods\n\nXpectrass provides 7+ normalization methods:\n- **SNV (Standard Normal Variate)**: Removes scatter effects\n- **Vector (L2)**: Unit length scaling\n- **Min-Max**: Scales to 0-1 range\n- **Area**: Normalizes total spectral area\n- **Peak**: Scales to maximum peak\n- **PQN (Probabilistic Quotient)**: Robust to dilution\n- **Entropy-weighted**: Information theory-based\n\n### Evaluation Strategy\n\nUnlike denoising and baseline correction, normalization is evaluated using **classification performance**:\n- **Cross-validation accuracy**: How well can we classify after normalization?\n- **Between-group separation**: Are different polymer types more distinguishable?\n- **Within-group consistency**: Are same-type polymers more similar?\n\nThe method that produces the best classification performance is typically the best choice.\n\n### Expected Output\n\n- Preprocessed data with denoising, baseline correction, and atmospheric correction\n- Classification accuracy for each normalization method\n- Excel file with ranked normalization methods\n- Recommendation for final preprocessing pipeline\n\n---\n\n## Step 1: Load Data and Define Parameters"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e944fd",
   "metadata": {},
   "outputs": [],
   "source": "# Import required modules\nimport polars as pl\nfrom xpectrass import FTIRdataprocessing\nfrom xpectrass import load_villegas_camacho_2024_c4\n\n# Load the dataset\ndataset = load_villegas_camacho_2024_c4()\nprint('Dataset shape:', dataset.shape)\n\n# Define the label column\nLABEL_COLUMN = \"type\"\n\n# Flat windows for baseline evaluation (from Notebook 2)\nFLAT_WINDOWS = [(1880, 1900), (2400, 2700)]\n\n# Selected methods from previous notebooks\nDENOISING_METHOD = 'wavelet'  # From Notebook 1\nBASELINE_CORRECTION_METHOD = 'aspls'  # From Notebook 2\n\n# Define atmospheric regions to handle\n# EXCLUDE_REGIONS: Completely remove these spectral regions\nEXCLUDE_REGIONS = [\n    (0, 680),       # Low wavenumber noise region + CO₂ bending (670 cm⁻¹)\n    (3500, 5000)    # High wavenumber noise region + O–H stretch\n]\n\n# INTERPOLATE_REGIONS: Replace these regions with interpolated values\n# This is useful for atmospheric interference regions (CO₂, H₂O)\nINTERPOLATE_REGIONS = [\n    (1250, 2700)    # H₂O bend region + CO₂ stretch region\n]\n\n# Interpolation method: \"zero\", \"linear\", or \"spline\"\n# \"zero\" sets the region to baseline (recommended for atmospheric correction)\nINTERPOLATE_METHOD = \"zero\"\n\nprint(f\"\\nPreprocessing parameters:\")\nprint(f\"  Denoising method: {DENOISING_METHOD}\")\nprint(f\"  Baseline method: {BASELINE_CORRECTION_METHOD}\")\nprint(f\"  Exclude regions: {EXCLUDE_REGIONS}\")\nprint(f\"  Interpolate regions: {INTERPOLATE_REGIONS}\")\nprint(f\"  Interpolation method: {INTERPOLATE_METHOD}\")\nprint(\"\\nFirst few rows of data:\")\nprint(dataset.head(5))"
  },
  {
   "cell_type": "markdown",
   "id": "8nb8015vlrv",
   "source": "### Understanding Atmospheric Correction\n\n**Why handle atmospheric interference?**\n- CO₂ and H₂O in the air between sample and detector create absorption peaks\n- These peaks are not from your sample and should be removed\n- Common atmospheric regions: 670 cm⁻¹ (CO₂), 1400-1900 cm⁻¹ (H₂O), 2350 cm⁻¹ (CO₂), 3500+ cm⁻¹ (H₂O)\n\n**Two approaches:**\n1. **EXCLUDE**: Completely remove regions (edges, noise)\n2. **INTERPOLATE**: Replace with baseline (atmospheric peaks)\n\n**Interpolation methods:**\n- `\"zero\"`: Set to zero/baseline (recommended for atmospheric regions)\n- `\"linear\"`: Linear interpolation between boundaries\n- `\"spline\"`: Smooth spline interpolation\n\n---\n\n## Step 2: Apply Full Preprocessing Pipeline",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a91dcf",
   "metadata": {},
   "outputs": [],
   "source": "# Initialize the preprocessing pipeline with all parameters\nfdp = FTIRdataprocessing(\n    df=dataset,\n    label_column=LABEL_COLUMN,\n    exclude_regions=EXCLUDE_REGIONS,\n    interpolate_regions=INTERPOLATE_REGIONS,\n    flat_windows=FLAT_WINDOWS\n)\n\nprint(\"Applying preprocessing pipeline:\")\nprint(\"  1. Convert to absorbance\")\nprint(\"  2. Denoise spectra\")\nprint(\"  3. Correct baseline\")\nprint(\"  4. Handle atmospheric interference\")\nprint(\"\\nThis may take a few minutes for large datasets...\\n\")\n\n# Apply full preprocessing: conversion + denoising + baseline + atmospheric correction\n# This is a helper method that combines multiple steps\ndf_corrected = fdp._get_atmosphere_corrected_data(\n    denoising_method=DENOISING_METHOD,\n    baseline_correction_method=BASELINE_CORRECTION_METHOD,\n    interpolate_method=INTERPOLATE_METHOD,\n    plot=True,  # Show before/after comparison\n)\n\nprint(f\"\\n✓ Preprocessing complete!\")\nprint(f\"Corrected data shape: {df_corrected.shape}\")\nprint(f\"Original features: {dataset.shape[1] - 1}\")\nprint(f\"After region exclusion: {df_corrected.shape[1] - 1}\")\n\n# Save the preprocessed (but not normalized) data\noutput_file = 'denoised_baseline_atmospheric_corrected_data.xlsx'\ndf_corrected.to_excel(output_file, index=False)\nprint(f\"\\n✓ Preprocessed data saved to: {output_file}\")"
  },
  {
   "cell_type": "markdown",
   "id": "wa8ymzkygy",
   "source": "### What Just Happened?\n\nThe pipeline applied multiple preprocessing steps in the correct order:\n\n1. **Conversion**: Transmittance → Absorbance\n2. **Denoising**: Applied your selected method (wavelet, savgol, etc.)\n3. **Baseline Correction**: Applied your selected method (asls, airpls, etc.)\n4. **Atmospheric Correction**: \n   - Excluded low/high wavenumber noise regions\n   - Interpolated over CO₂ and H₂O regions\n\n**Important**: The data is now ready for normalization evaluation!\n\n---\n\n## Step 3: Evaluate Normalization Methods\n\nNow we'll evaluate all normalization methods using **classification performance**. The method that produces the best classification accuracy is typically the best choice for your preprocessing pipeline.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4c77b2",
   "metadata": {},
   "outputs": [],
   "source": "# Import data handling libraries\nimport pandas as pd\nimport polars as pl\n\nprint(\"=\"*80)\nprint(\"Evaluating normalization methods using classification performance\")\nprint(\"=\"*80)\n\n# Check data format\nis_polars = isinstance(df_corrected, pl.DataFrame)\n\n# Get unique polymer types\nif is_polars:\n    labels = df_corrected.get_column(LABEL_COLUMN).unique().sort().to_list()\nelse:\n    labels = df_corrected[LABEL_COLUMN].sort_values().unique().tolist()\n\nprint(f\"\\nFound {len(labels)} polymer types: {labels}\")\nprint(\"\\nThis evaluation uses cross-validation to test classification performance\")\nprint(\"with each normalization method. This may take several minutes...\\n\")\n\n# Store results for all polymer types\nall_results = []\n\n# Evaluate normalization methods\n# Note: We evaluate on the full dataset, not per polymer type\n# This is because normalization quality is measured by classification performance\nfor i, label in enumerate(labels, 1):\n    print(f\"{'='*80}\")\n    print(f\"[{i}/{len(labels)}] Evaluating for polymer type: {label}\")\n    print(f\"{'='*80}\\n\")\n    \n    # Filter data for current polymer type (for reporting purposes)\n    if is_polars:\n        df_label = df_corrected.filter(pl.col(LABEL_COLUMN) == label)\n    else:\n        df_label = df_corrected[df_corrected[LABEL_COLUMN] == label]\n    \n    print(f\"Number of {label} samples: {len(df_label)}\\n\")\n    \n    # Evaluate normalization methods using classification\n    # data=df_corrected: Use full dataset (not just one polymer type)\n    # methods=\"FTIR\": Test FTIR-specific normalization methods\n    # n_splits=5: Use 5-fold cross-validation for robust evaluation\n    result = fdp.find_normalization_method(\n        data=df_corrected,  # Full dataset for classification\n        methods=\"FTIR\",\n        n_splits=5,\n    )\n    \n    # Add polymer type label to results (for tracking purposes)\n    result = result.copy()\n    result[\"type\"] = label\n    all_results.append(result)\n    \n    # Display top method\n    top_method = result.iloc[0]\n    print(f\"\\n✓ Top normalization method for {label}: {top_method['method']}\")\n    print(f\"  Classification accuracy: {top_method.get('accuracy', 'N/A')}\")\n\n# Combine results from all iterations\nprint(\"\\n\" + \"=\"*80)\nprint(\"Saving combined results...\")\nprint(\"=\"*80)\n\nfinal_df = pd.concat(all_results, ignore_index=True)\n\n# Save results to Excel\noutput_file = 'normalization_methods_scores.xlsx'\nfinal_df.to_excel(output_file, index=False)\n\nprint(f\"\\n✓ Results saved to: {output_file}\")\nprint(f\"Total evaluations: {len(final_df)}\")\n\n# Display summary of top methods\nprint(\"\\n\" + \"=\"*80)\nprint(\"SUMMARY: Recommended normalization methods\")\nprint(\"=\"*80)\n\n# Group by method and calculate average performance\nif 'accuracy' in final_df.columns:\n    method_summary = final_df.groupby('method')['accuracy'].mean().sort_values(ascending=False)\n    print(\"\\nAverage classification accuracy by method:\")\n    for method, accuracy in method_summary.head(5).items():\n        print(f\"  {method:20s} → {accuracy:.4f}\")\nelse:\n    print(\"\\nTop method from first evaluation:\")\n    print(f\"  {final_df.iloc[0]['method']}\")"
  },
  {
   "cell_type": "markdown",
   "id": "gqifm52b0kh",
   "source": "### Understanding the Results\n\nThe evaluation process:\n\n1. **Tests each normalization method** on the preprocessed data\n2. **Runs classification** using a simple classifier (e.g., Random Forest)\n3. **Uses cross-validation** (5 folds) for robust accuracy estimation\n4. **Ranks methods** by classification performance\n\n### How to Interpret the Metrics\n\nUnlike denoising and baseline correction, normalization is evaluated by:\n\n- **Classification Accuracy**: \n  - Higher is better\n  - Measures how well polymer types can be distinguished after normalization\n  - The method with highest accuracy typically produces the best separation\n\n- **Between-group vs Within-group variance**:\n  - Good normalization maximizes differences between polymer types\n  - While minimizing variance within each polymer type\n\n### Common Top Methods\n\n- **SNV (Standard Normal Variate)**: \n  - Often performs best for FTIR data\n  - Removes scatter effects effectively\n  - Recommended for plastic classification\n\n- **Vector (L2 normalization)**:\n  - Scales to unit length\n  - Good when intensity varies but spectral shapes are important\n\n- **Min-Max**:\n  - Scales to 0-1 range\n  - Good for visualization and some ML algorithms\n\n- **Area normalization**:\n  - Normalizes total absorbance\n  - Good for concentration-independent comparison\n\n### Evaluation Note\n\nThe loop iterates through polymer types for reporting purposes, but evaluates on the **full dataset** each time. This is because normalization quality is best measured by how well it improves classification of all polymer types together.\n\n---\n\n## Next Steps\n\n1. **Review the Excel file** `normalization_methods_scores.xlsx`\n2. **Choose the method with highest classification accuracy**\n3. **Apply it in your final preprocessing pipeline**:\n   ```python\n   fdp.normalize(method=\"snv\")  # Replace \"snv\" with your chosen method\n   ```\n\n### Complete Preprocessing Pipeline\n\nOnce you've selected your normalization method, your complete pipeline is:\n\n```python\nfrom xpectrass import FTIRdataprocessing\n\nfdp = FTIRdataprocessing(df, label_column=\"type\")\n\n# Step 1: Convert\nfdp.convert(mode=\"to_absorbance\")\n\n# Step 2: Denoise\nfdp.denoise_spect(method=\"wavelet\")  # Your selected method\n\n# Step 3: Baseline correction\nfdp.correct_baseline(method=\"aspls\")  # Your selected method\n\n# Step 4: Atmospheric correction\nfdp.exclude_interpolate(method=\"zero\")\n\n# Step 5: Normalize\nfdp.normalize(method=\"snv\")  # Your selected method\n\n# Get final processed data\nprocessed_data = fdp.df_norm\n```\n\n---\n\n## Conclusion\n\nYou've completed the method selection process! You now know the best denoising, baseline correction, and normalization methods for your data. Proceed to Notebook 4 to apply these methods to process your dataset.",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ToF-SIMS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}